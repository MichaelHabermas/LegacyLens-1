# LegacyLens Tech Stack Options

This document elaborates on possible tech stack combinations for building the LegacyLens RAG system, as outlined in the Pre-Search Report. It incorporates decisions from the project specification (G4 Week 2 - LegacyLens.pdf) and the updated Pre-Search Checklist (PRE-SEARCH-CHECKLIST.md). Key influences include:

- **Project Requirements**: Focus on syntax-aware chunking for legacy languages (e.g., COBOL paragraphs, Fortran subroutines), semantic search, code understanding features (e.g., dependency mapping, impact analysis), <3s query latency, >70% retrieval precision, and scalability from MVP (24 hours) to production (100-100K users).
- **Pre-Search Decisions**: Prioritize code-optimized embeddings (e.g., Voyage-code-3), hybrid search for exact terms, GraphRAG for dependencies, agentic workflows for advanced features, and cost tradeoffs (e.g., managed vs. self-hosted). Use Python for core development due to libraries like dendropy/biopython for AST parsing in legacy code.
- **2026 Updates**: Incorporate newer models (e.g., Jina Embeddings v4, Zerank-2 reranker), open-source LLMs (e.g., DeepSeek-V3.2), and frameworks with agentic support (e.g., LlamaIndex Workflows 1.0, LangGraph v1.0).
- **Rationale for Elaboration**: Stacks are grouped by focus (prototyping, production, advanced). Each includes detailed components, alignment with MVP/early/final deadlines, estimated costs, and how it addresses failure modes (e.g., no-results fallback via agentic rerouting). All stacks support 100% codebase coverage, batch/incremental ingestion, and public deployment (e.g., Vercel/Railway).

Stacks are presented in tables for clarity, followed by pros/cons and fit analysis.

## Prototyping-Focused Stacks

These emphasize low-cost, local development for rapid MVP iteration (e.g., basic ingestion and retrieval in 24 hours). Ideal for testing chunking strategies and retrieval precision before scaling.

| Stack                | Language/Framework                                                 | Vector DB                                    | Embedding Model                                                                                 | LLM                                                                                  | Deployment                                                         | Best For                                                                                                                                                                                                                              | Estimated Dev Cost                  | Production Projections (100/1K/10K/100K Users)                                                  |
| -------------------- | ------------------------------------------------------------------ | -------------------------------------------- | ----------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------ | ------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------- | ----------------------------------------------------------------------------------------------- |
| 1. Local Open-Source | Python + LlamaIndex (with LlamaParse v2 for syntax-aware chunking) | ChromaDB (embedded, local)                   | nomic-embed-text-v1 (local, 768 dims, free; or Jina Embeddings v4 for multimodal code/comments) | DeepSeek-V3.2 (self-hosted, MIT license; strong code reasoning at 82% on benchmarks) | Local CLI (Streamlit for quick web interface) or Railway free tier | MVP prototyping; local testing of COBOL/Fortran chunking (e.g., function-level via AST); no API dependencies for open-source codebases like GnuCOBOL. Supports hierarchical chunking (file → section → function) with 10-25% overlap. | $0 (all local/open-source)          | $0 / $5 / $20 / $100 (self-hosting scales with hardware; add Redis for caching at higher users) |
| 2. Budget Hybrid     | Python + LangChain (with LangGraph v1.0 for agentic loops)         | pgvector (PostgreSQL extension, self-hosted) | bge-base-en-v1.5 or E5-Large-V2 (local, free; batch processing with content-hash caching)       | GLM-4.7 (self-hosted; excels in structured output for citations/file refs)           | Railway (~$5/mo starter) or local Docker                           | Incremental updates via GitHub Actions; SQL-based metadata filtering (e.g., by file path/line numbers); good for pattern detection and bug search features. Integrates Neo4j for basic GraphRAG dependencies.                         | <$5 (free tiers + local embeddings) | $5 / $20 / $100 / $500 (Postgres hosting scales; low token costs with open-source LLM)          |

**Elaboration**: These stacks align with the project's "get basic retrieval working before optimizing" guidance. For example, ChromaDB/pgvector support quick insertion/verification of embeddings, enabling early testing of scenarios like "Where is the main entry point?" Local models reduce latency to <1s for dev queries and avoid external API costs, fitting the budget constraints in Phase 1. Failure modes (e.g., ambiguous queries) are handled via prompt-based query expansion in LangChain. Tradeoff: Limited to millions of vectors; not ideal for 10K+ users without upgrades.

## Production-Focused Stacks

These prioritize managed services for scalability, low latency, and production features like hybrid search and reranking. Suitable for early submission (full features by Friday) and final polish (deployment by Sunday).

| Stack                               | Language/Framework                                                               | Vector DB                                                             | Embedding Model                                                                                                                    | LLM                                                                                | Deployment                        | Best For                                                                                                                                                                                                   | Estimated Dev Cost                 | Production Projections (100/1K/10K/100K Users)                                              |
| ----------------------------------- | -------------------------------------------------------------------------------- | --------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- | --------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------- | ------------------------------------------------------------------------------------------- |
| 3. Managed Cloud (Chosen in Report) | Python + LlamaIndex (Workflows 1.0 + LlamaAgents Builder for agentic refinement) | Pinecone (serverless, managed cloud; free tier up to 2GB)             | Voyage-code-3 (API, 1536 dims; 14-20% better code recall; 200M free tokens)                                                        | Claude 4.5 Sonnet (API; 87% on LiveCodeBench for code explanation/impact analysis) | Vercel (serverless; free for MVP) | High-scale queries (<3s end-to-end); hybrid search for exact terms (e.g., variable names); agentic loops for 4+ features like dependency mapping and translation hints. Supports re-ranking with Zerank-2. | <$10 (free tiers + initial tokens) | $5 / $50 / $500 / $5,000 (assumes 5 queries/user/day, 2K tokens/query; scales serverlessly) |
| 4. Enterprise Scale                 | Python + LangChain (LangGraph v1.0 for multi-query expansion)                    | Qdrant (managed/hybrid cloud; free 1GB cluster; Rust-based for speed) | OpenAI text-embedding-3-large (3072 dims; Matryoshka for cost reduction) or Jina Code Embeddings v2 (8192 tokens for large chunks) | GPT-5.2 Codex (API; 89% on coding benchmarks; structured citations)                | Render (~$20/mo) or AWS (for GPU) | Large codebases (e.g., LAPACK with 10K+ LOC); GPU acceleration for ingestion throughput (<5 min); reranking for >70% top-5 precision. Ideal for business logic extraction.                                 | $10-20 (API tokens + hosting)      | $20 / $100 / $1,000 / $10,000 (higher due to OpenAI costs; optimize with batching)          |

**Elaboration**: Based on the project's vector DB recommendations (Pinecone for production scale, Qdrant for filtering), these stacks ensure 100% coverage and correct file/line references. Managed hosting trades money for time (per Phase 1), enabling focus on chunking refinement (e.g., semantic splitting with LLM boundaries). Claude/GPT handle answer generation with low hallucination, fitting performance targets. For production costs, assumptions include occasional code additions (1% monthly re-embedding) and observability via LangSmith/Phoenix. Tradeoff: Vendor lock-in and API costs; mitigated by free tiers for dev.

## Advanced/Agentic Stacks

These extend to GraphRAG and agentic workflows for complex features (e.g., impact analysis, bug patterns). Best for final submission, adding polish like observability and CI/CD.

| Stack                | Language/Framework                                                                             | Vector DB                                           | Embedding Model                                                    | LLM                                                                    | Deployment                              | Best For                                                                                                                                                      | Estimated Dev Cost           | Production Projections (100/1K/10K/100K Users)                                       |
| -------------------- | ---------------------------------------------------------------------------------------------- | --------------------------------------------------- | ------------------------------------------------------------------ | ---------------------------------------------------------------------- | --------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------- | ------------------------------------------------------------------------------------ |
| 5. GraphRAG Enhanced | Python + LlamaIndex (LlamaAgents Builder + Neo4j integration for graphs)                       | Weaviate (cloud; GraphQL API for hybrid search)     | Qwen3-Embedding (open-source, multilingual code; or Voyage-code-3) | Gemini 3 Pro (API; long-context for multi-file dependencies)           | AWS/Heroku (~$20-50/mo)                 | Dependency mapping and error handling patterns; GraphRAG for relationships (e.g., "What affects MODULE-X?"); hierarchical chunking with graph metadata.       | $15-30 (API + graph hosting) | $10 / $100 / $1,000 / $10,000 (graph ops add cost; scale with Zilliz BYOC)           |
| 6. Custom Agentic    | Node.js + LangChain (LangGraph v1.0 for custom agents) + FastAPI (Python backend for chunking) | Milvus/Zilliz (GPU-accelerated, self-host or cloud) | CodeSage Large V2 (code retrieval specialist; local/API hybrid)    | Claude Opus 4.5 (API; excellent for debugging and modernization hints) | Vercel Edge Functions (for low-latency) | Full control over pipelines; multi-query for ambiguous tests (e.g., "Find all file I/O"); integration with evaluation tools like RAGAS for precision metrics. | $20-40 (GPU + APIs)          | $50 / $200 / $2,000 / $20,000 (GPU scales for large queries; high for agentic loops) |

**Elaboration**: Drawing from 2026 trends in the checklist (e.g., GraphRAG for dependencies, agentic loops for refinement), these stacks implement at least 4 code understanding features. Weaviate/Milvus support large-scale with GPU, aligning with ingestion throughput targets. Custom elements allow failure mode handling (e.g., rate limiting via ops runbooks) and observability (e.g., alerting on latency >3s). Deployment includes CI/CD for index updates, per Phase 3. Tradeoff: Higher complexity and learning curve; recommended only after basic retrieval is solid.

## Summary of Tradeoffs and Recommendations

- **Performance vs. Cost**: Local stacks (1-2) minimize costs but cap scalability; managed (3-4) offer <100ms latency at higher fees.
- **Scalability vs. Complexity**: Production stacks auto-scale but require API keys/secrets management; advanced (5-6) add agents/graphs for precision but risk infinite loops.
- **Fit to Decisions**: All prioritize Python for legacy parsing (e.g., biopython for Fortran). Chosen Stack 3 best balances MVP speed with production projections, as per report rationale. For proprietary code, switch to self-hosted models to meet data residency.
- **Query Interface**: The project will use a **CLI** as the query interface (not a web UI). The spec allows CLI or web; CLI is chosen to limit scope and align with a Python backend. Required behaviors (natural language input, snippets with syntax highlighting, file/line refs, scores, LLM answer, drill-down) will be implemented in the CLI when built later (building is a later effort, not this one).
- **Next Steps**: Complete Pre-Search phases to refine (e.g., test chunk sizes 512-8192 tokens). Document in RAG Architecture Doc.
